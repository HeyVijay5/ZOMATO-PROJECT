{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "YJ55k-q6phqO",
        "x-EpHcCOp1ci",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeyVijay5/ZOMATO-PROJECT/blob/main/ZOMATO_PROJECT_LABMENTIX_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ZOMATO - DATA SCIENCE**     \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The food delivery and restaurant discovery industry has witnessed rapid growth due to increasing urbanization and changing consumer preferences. Platforms like Zomato generate large volumes of data in the form of restaurant metadata and customer reviews, which can be effectively analyzed to extract valuable insights. This project aims to perform a comprehensive data science analysis on Zomato restaurant and review data to understand customer sentiment, restaurant performance, and emerging patterns using machine learning techniques.\n",
        "\n",
        "The dataset used in this project consists of two primary components. The first dataset contains restaurant-level metadata such as restaurant names, cost information, cuisines offered, collections, and operational details. The second dataset comprises customer review data, including reviewer details, textual reviews, ratings, and timestamps. These datasets together provide a holistic view of both restaurant characteristics and customer perceptions.\n",
        "\n",
        "The project begins with Exploratory Data Analysis (EDA) to understand the structure, quality, and distribution of data. Key statistical summaries and visualizations are used to analyze rating distributions, cost patterns, and review characteristics. Missing values, inconsistencies, and duplicates are identified during this phase, enabling informed decisions during data cleaning. EDA helps in uncovering initial trends such as the relationship between restaurant cost and customer ratings, as well as identifying commonly preferred cuisines.\n",
        "\n",
        "Data cleaning and preprocessing form a critical part of the workflow. This includes handling missing values, removing duplicates, standardizing restaurant names, and cleaning textual review data by eliminating noise such as special characters and unnecessary symbols. Feature engineering techniques are applied to derive meaningful attributes from the raw data, including sentiment labels derived from ratings and numerical features representing review characteristics.\n",
        "\n",
        "Sentiment analysis is a core component of this project. Customer ratings are converted into four sentiment categories: Highly Positive, Positive, Neutral, and Negative. A supervised machine learning classification model is trained using review text as input features and sentiment labels as the target variable. Text vectorization techniques are applied to convert unstructured text into numerical representations suitable for model training. The performance of the classification model is evaluated using accuracy, precision, recall, and F1-score metrics.\n",
        "\n",
        "In addition to sentiment classification, unsupervised learning techniques are employed to cluster restaurants based on their attributes such as cost, cuisine diversity, and aggregated sentiment scores. Clustering enables the identification of distinct groups of restaurants with similar characteristics, helping in understanding market segmentation and competitive positioning.\n",
        "\n",
        "The final phase of the project focuses on the visualization and interpretation of results. Insights derived from the analysis are presented using meaningful visualizations that highlight customer preferences, high-performing restaurants, and areas requiring improvement. These insights can be valuable for customers seeking better dining options, restaurant owners aiming to improve service quality, and platform providers looking to enhance recommendation systems.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here.\n",
        "https://github.com/HeyVijay5/ZOMATO-PROJECT\n",
        "\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Online food discovery platforms such as Zomato host a large amount of data related to restaurants and customer reviews. While this data is rich in information, it is often unstructured and underutilized. Customers face difficulty in identifying high-quality restaurants, and restaurant owners lack clear insights into customer perceptions and areas of improvement.\n",
        "\n",
        "The objective of this project is to analyze Zomato restaurant metadata and customer review data to extract meaningful insights using data science and machine learning techniques. The project aims to understand customer sentiment based on ratings and review text, evaluate restaurant performance, and identify patterns that influence customer satisfaction.\n",
        "\n",
        "Specifically, the problem involves performing exploratory data analysis to understand rating distributions and restaurant characteristics, classifying customer reviews into sentiment categories, and clustering restaurants based on their attributes. By combining structured restaurant data with unstructured textual reviews, the project seeks to provide data-driven insights that can support better decision-making for customers, restaurants, and platform stakeholders"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import Required Libraries\n",
        "\n",
        "# Data manipulation and numerical computation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Ignore warnings for clean output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set visualization style\n",
        "sns.set(style=\"whitegrid\")\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Loading (Manual Upload - Two Excel Files)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "try:\n",
        "    # Upload both Excel files manually\n",
        "    uploaded_files = files.upload()\n",
        "\n",
        "    # Load restaurant metadata dataset\n",
        "    restaurants_df = pd.read_excel(\"Zomato Restaurant names and Metadata.xlsx\")\n",
        "\n",
        "    # Load reviews dataset\n",
        "    reviews_df = pd.read_excel(\"Zomato Restaurant reviews.xlsx\")\n",
        "\n",
        "    print(\"Both datasets uploaded and loaded successfully!\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(\"Error: Please ensure both Excel files are uploaded with correct names.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Dataset First View\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    # Load datasets using exact file names\n",
        "    restaurants_df = pd.read_csv(\"Zomato Restaurant names and Metadata.csv\")\n",
        "    reviews_df = pd.read_csv(\"Zomato Restaurant reviews.csv\")\n",
        "\n",
        "    # Display first 5 rows of Restaurant Metadata dataset\n",
        "    print(\"First View: Zomato Restaurant Names and Metadata Dataset\")\n",
        "    display(restaurants_df.head())\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Display first 5 rows of Restaurant Reviews dataset\n",
        "    print(\"First View: Zomato Restaurant Reviews Dataset\")\n",
        "    display(reviews_df.head())\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(\"File not found. Please check the exact file names.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Dataset Rows & Columns Count\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    # Rows and columns count for Restaurant Metadata dataset\n",
        "    print(\"Zomato Restaurant Names and Metadata Dataset:\")\n",
        "    print(f\"Number of Rows: {restaurants_df.shape[0]}\")\n",
        "    print(f\"Number of Columns: {restaurants_df.shape[1]}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
        "\n",
        "    # Rows and columns count for Restaurant Reviews dataset\n",
        "    print(\"Zomato Restaurant Reviews Dataset:\")\n",
        "    print(f\"Number of Rows: {reviews_df.shape[0]}\")\n",
        "    print(f\"Number of Columns: {reviews_df.shape[1]}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while fetching dataset shape information.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Dataset Information\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    print(\"Zomato Restaurant Names and Metadata Dataset Info:\\n\")\n",
        "    restaurants_df.info()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"Zomato Restaurant Reviews Dataset Info:\\n\")\n",
        "    reviews_df.info()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while retrieving dataset information.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Dataset Duplicate Value Count\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    # Duplicate count in Restaurant Metadata dataset\n",
        "    metadata_duplicates = restaurants_df.duplicated().sum()\n",
        "    print(\"Zomato Restaurant Names and Metadata Dataset:\")\n",
        "    print(f\"Number of Duplicate Rows: {metadata_duplicates}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
        "\n",
        "    # Duplicate count in Restaurant Reviews dataset\n",
        "    reviews_duplicates = reviews_df.duplicated().sum()\n",
        "    print(\"Zomato Restaurant Reviews Dataset:\")\n",
        "    print(f\"Number of Duplicate Rows: {reviews_duplicates}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while checking duplicate values.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Missing Values / Null Values Count\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    print(\"Missing Values Count - Zomato Restaurant Names and Metadata Dataset:\\n\")\n",
        "    print(restaurants_df.isnull().sum())\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"Missing Values Count - Zomato Restaurant Reviews Dataset:\\n\")\n",
        "    print(reviews_df.isnull().sum())\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while checking missing values.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Visualizing Missing Values\n",
        "# ================================\n",
        "\n",
        "# Set figure size for better readability\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Heatmap for missing values in Restaurant Metadata dataset\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(restaurants_df.isnull(), cbar=False, cmap=\"viridis\")\n",
        "plt.title(\"Missing Values Heatmap - Restaurant Metadata\")\n",
        "\n",
        "# Heatmap for missing values in Restaurant Reviews dataset\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(reviews_df.isnull(), cbar=False, cmap=\"viridis\")\n",
        "plt.title(\"Missing Values Heatmap - Restaurant Reviews\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project uses two datasets: a restaurant metadata dataset containing information about 105 restaurants with 6 attributes, and a restaurant reviews dataset consisting of 10,000 customer reviews with 7 attributes. The metadata dataset provides structured information such as restaurant names, cost, cuisines, collections, and timings, while the reviews dataset contains unstructured textual data along with ratings and reviewer details. Together, these datasets offer a comprehensive view of both restaurant characteristics and customer perceptions.\n",
        "\n",
        "Initial exploration shows that most columns in both datasets are of object type, indicating categorical and text-based information. The reviews dataset includes free-text reviews, which makes it suitable for sentiment analysis, while numerical information such as ratings and picture counts supports quantitative analysis. The rating column is currently stored as a categorical variable and will require conversion before further analysis.\n",
        "\n",
        "The missing value analysis reveals that the restaurant metadata dataset has a significant number of missing values in the Collections column, while the Timings column has only one missing entry. All other columns in this dataset are complete. In contrast, the reviews dataset contains very few missing values across multiple columns, and these missing entries appear to be randomly distributed. This suggests that the overall data quality is high and suitable for modeling after minimal cleaning.\n",
        "\n",
        "Duplicate analysis indicates that there are no duplicate records in the restaurant metadata dataset, while the reviews dataset contains 36 duplicate rows. These duplicates need to be removed to prevent bias during sentiment classification and statistical analysis.\n",
        "\n",
        "Overall, the dataset is well-structured, sufficiently large, and rich in information. With appropriate data cleaning and preprocessing, it provides a strong foundation for exploratory data analysis, sentiment classification, and restaurant clustering. The insights derived from this data can support data-driven decision-making for customers, restaurants, and platform stakeholders."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Dataset Columns\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    print(\"Columns in Zomato Restaurant Names and Metadata Dataset:\\n\")\n",
        "    for col in restaurants_df.columns:\n",
        "        print(f\"- {col}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"Columns in Zomato Restaurant Reviews Dataset:\\n\")\n",
        "    for col in reviews_df.columns:\n",
        "        print(f\"- {col}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while displaying dataset columns.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Dataset Describe\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    print(\"Statistical Summary - Zomato Restaurant Names and Metadata Dataset:\\n\")\n",
        "    display(restaurants_df.describe(include='all'))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"Statistical Summary - Zomato Restaurant Reviews Dataset:\\n\")\n",
        "    display(reviews_df.describe(include='all'))\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while generating dataset description.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of both structured and unstructured variables representing restaurant characteristics and customer feedback. The restaurant metadata variables capture essential business attributes such as restaurant name, cost, cuisine types, operational timings, and curated collections, which are useful for understanding restaurant positioning and service offerings. These variables are primarily categorical in nature and support exploratory and segmentation analysis.\n",
        "\n",
        "The reviews dataset contains customer-generated information, including reviewer identity, textual reviews, ratings, and engagement indicators. The review text represents unstructured data and serves as the primary input for sentiment analysis, while the rating variable provides a quantitative measure of customer satisfaction. Additional variables such as review metadata, time of review, and picture counts help in analyzing reviewer behavior and engagement patterns.\n",
        "\n",
        "Overall, the combination of metadata and review-based variables enables a comprehensive analysis of restaurant performance and customer perception, supporting both descriptive analysis and machine learningâ€“based modeling."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Check Unique Values for Each Variable\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    print(\"Unique Values Count - Zomato Restaurant Names and Metadata Dataset:\\n\")\n",
        "    for col in restaurants_df.columns:\n",
        "        print(f\"{col}: {restaurants_df[col].nunique()} unique values\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"Unique Values Count - Zomato Restaurant Reviews Dataset:\\n\")\n",
        "    for col in reviews_df.columns:\n",
        "        print(f\"{col}: {reviews_df[col].nunique()} unique values\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while checking unique values.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Data Wrangling\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    # ----------------------------\n",
        "    # 1. Remove duplicate rows\n",
        "    # ----------------------------\n",
        "    reviews_df = reviews_df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 2. Handle missing values\n",
        "    # ----------------------------\n",
        "\n",
        "    # Restaurant Metadata Dataset\n",
        "    # Fill missing Collections with 'Not Specified'\n",
        "    restaurants_df['Collections'] = restaurants_df['Collections'].fillna('Not Specified')\n",
        "\n",
        "    # Fill missing Timings with 'Not Available'\n",
        "    restaurants_df['Timings'] = restaurants_df['Timings'].fillna('Not Available')\n",
        "\n",
        "    # Reviews Dataset\n",
        "    # Drop rows where Review or Rating is missing (critical for sentiment analysis)\n",
        "    reviews_df = reviews_df.dropna(subset=['Review', 'Rating'])\n",
        "\n",
        "    # Fill remaining missing values with 'Unknown'\n",
        "    reviews_df['Reviewer'] = reviews_df['Reviewer'].fillna('Unknown')\n",
        "    reviews_df['Metadata'] = reviews_df['Metadata'].fillna('Unknown')\n",
        "    reviews_df['Time'] = reviews_df['Time'].fillna('Unknown')\n",
        "\n",
        "    # ----------------------------\n",
        "    # 3. Data type conversion\n",
        "    # ----------------------------\n",
        "\n",
        "    # Convert Rating column to numeric\n",
        "    reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')\n",
        "\n",
        "    # Drop rows where Rating conversion failed\n",
        "    reviews_df = reviews_df.dropna(subset=['Rating'])\n",
        "\n",
        "    # ----------------------------\n",
        "    # 4. Text cleaning preparation\n",
        "    # ----------------------------\n",
        "\n",
        "    # Convert review text to string and lowercase\n",
        "    reviews_df['Review'] = reviews_df['Review'].astype(str).str.lower()\n",
        "\n",
        "    print(\"Data wrangling completed successfully!\")\n",
        "    print(\"Cleaned Restaurant Metadata Shape:\", restaurants_df.shape)\n",
        "    print(\"Cleaned Restaurant Reviews Shape:\", reviews_df.shape)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during data wrangling.\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the data wrangling phase, duplicate records were removed from the reviews dataset to prevent bias in analysis, and missing values were handled using context-appropriate strategies. In the restaurant metadata dataset, missing values in the Collections and Timings columns were filled with meaningful placeholders to preserve restaurant information. In the reviews dataset, rows with missing reviews or ratings were removed as they are critical for sentiment analysis, while remaining missing reviewer and metadata fields were handled using default values. The rating variable was converted from categorical to numeric format to enable quantitative analysis and modeling. These manipulations resulted in clean, consistent datasets with 105 restaurants and 9,954 valid reviews, making the data analysis-ready and suitable for reliable exploratory analysis and machine learning tasks."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 1** **Distribution of Customer Ratings**\n",
        "\n",
        "(Univariate Analysis)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 1: Distribution of Customer Ratings\n",
        "# ================================\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "sns.countplot(\n",
        "    x='Rating',\n",
        "    data=reviews_df,\n",
        "    order=sorted(reviews_df['Rating'].unique()),\n",
        "    palette='viridis'\n",
        ")\n",
        "\n",
        "plt.title(\"Distribution of Customer Ratings\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A count plot is ideal for visualizing the frequency distribution of a discrete categorical variable like ratings. It clearly shows how reviews are spread across different rating levels and helps identify dominant customer sentiment patterns."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that customer ratings are heavily skewed toward the higher end of the scale, with ratings of 4 and 5 accounting for the majority of reviews. This indicates that most customers have a positive experience with the restaurants listed on the platform. Lower ratings (1 and 2) occur less frequently but are still present in notable numbers, suggesting that while overall satisfaction is high, there are specific cases of poor customer experience that warrant further investigation."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from this chart can help create a **positive business impact** by highlighting that the majority of customers give high ratings (4 and 5), which reflects strong overall customer satisfaction and trust in the platform. This information can be leveraged by Zomato to promote top-rated restaurants, strengthen recommendation systems, and attract new users by showcasing consistent positive experiences. High ratings also encourage restaurant partners to maintain service quality.\n",
        "\n",
        "However, the presence of a noticeable number of low ratings (1 and 2) points to **potential negative growth factors** if left unaddressed. These low ratings may indicate issues related to food quality, service, or delivery experience, which can harm customer retention and brand perception. If such negative feedback is ignored, it could lead to customer churn and reduced platform credibility. Identifying and addressing the causes behind these low ratings is therefore critical to minimizing negative business impact.\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 2 Distribution of Restaurant Cost**\n",
        "\n",
        "(Univariate Analysis)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 2: Distribution of Restaurant Cost\n",
        "# ================================\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "sns.countplot(\n",
        "    y='Cost',\n",
        "    data=restaurants_df,\n",
        "    order=restaurants_df['Cost'].value_counts().index,\n",
        "    palette='magma'\n",
        ")\n",
        "\n",
        "plt.title(\"Distribution of Restaurants by Cost Category\")\n",
        "plt.xlabel(\"Number of Restaurants\")\n",
        "plt.ylabel(\"Cost for Two\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal count plot is best suited for visualizing categorical variables with many distinct values, such as restaurant cost categories. It improves readability, allows easy comparison across categories, and clearly shows how restaurants are distributed across different price ranges."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most restaurants fall into the low-to-mid cost range, indicating a strong focus on affordability. Higher cost categories have fewer restaurants, suggesting that premium dining options are limited compared to budget and mid-range offerings."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This insight supports a positive business impact by confirming that the platform caters well to price-sensitive customers, which can drive higher user engagement and order frequency. However, the relatively low presence of high-cost restaurants may limit appeal to premium customers, potentially restricting growth in higher-margin segments."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 3 Top 10 Most Common Cuisines**\n",
        "\n",
        "(Univariate Analysis | Bar Chart)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 3: Top 10 Most Common Cuisines\n",
        "# ================================\n",
        "\n",
        "# Split cuisines and count frequency\n",
        "cuisine_series = restaurants_df['Cuisines'].str.split(', ').explode()\n",
        "\n",
        "top_cuisines = cuisine_series.value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "sns.barplot(\n",
        "    x=top_cuisines.values,\n",
        "    y=top_cuisines.index,\n",
        "    palette='coolwarm'\n",
        ")\n",
        "\n",
        "plt.title(\"Top 10 Most Common Cuisines\")\n",
        "plt.xlabel(\"Number of Restaurants\")\n",
        "plt.ylabel(\"Cuisine Type\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is appropriate for comparing the frequency of categorical variables such as cuisines. It clearly highlights the most popular cuisine types and allows easy comparison of restaurant counts across different cuisines."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that North Indian and Chinese cuisines dominate the restaurant landscape, followed by Continental and Biryani. This indicates strong customer demand and restaurant preference for these cuisine types, while cuisines like Bakery and South Indian have relatively fewer outlets."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dominance of popular cuisines supports positive business impact by aligning offerings with customer preferences, increasing engagement and order volume. However, over-reliance on a few cuisine types may limit variety and reduce opportunities to attract niche customer segments, potentially constraining diversification and long-term growth."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 4 Rating Distribution by Number of Pictures**\n",
        "\n",
        "(Bivariate Analysis | Box Plot)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 4: Rating vs Pictures (Customer Engagement)\n",
        "# ================================\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "sns.boxplot(\n",
        "    x='Pictures',\n",
        "    y='Rating',\n",
        "    data=reviews_df,\n",
        "    palette='Set2'\n",
        ")\n",
        "\n",
        "plt.title(\"Customer Ratings by Number of Pictures Uploaded\")\n",
        "plt.xlabel(\"Number of Pictures Uploaded\")\n",
        "plt.ylabel(\"Rating\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot is ideal for analyzing the relationship between two variables by showing the distribution, median, spread, and outliers. It helps compare how customer ratings vary across different levels of user engagement measured by the number of pictures uploaded."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart indicates that reviews with a higher number of uploaded pictures generally have higher median ratings, suggesting a positive association between customer engagement and satisfaction. Lower engagement levels show wider variability in ratings, including more low-rating outliers."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This insight has a positive business impact as it suggests that highly engaged customers (who upload pictures) tend to be more satisfied, and encouraging photo uploads could improve review quality and platform credibility. However, the presence of low-rating outliers among low-engagement users indicates potential negative experiences that, if ignored, could affect customer trust and retention."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 5 Customer Rating Distribution Across Cost Categories**\n",
        "\n",
        "(Bivariate Analysis | Violin Plot)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 5: Rating vs Cost Category\n",
        "# ================================\n",
        "\n",
        "# Merge reviews with restaurant metadata to bring Cost information\n",
        "merged_df = reviews_df.merge(\n",
        "    restaurants_df[['Name', 'Cost']],\n",
        "    left_on='Restaurant',\n",
        "    right_on='Name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "sns.violinplot(\n",
        "    x='Cost',\n",
        "    y='Rating',\n",
        "    data=merged_df,\n",
        "    inner='quartile',\n",
        "    palette='Spectral'\n",
        ")\n",
        "\n",
        "plt.title(\"Distribution of Customer Ratings Across Cost Categories\")\n",
        "plt.xlabel(\"Cost for Two\")\n",
        "plt.ylabel(\"Rating\")\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A violin plot is effective for visualizing the full distribution of ratings across different cost categories, as it combines density, spread, and central tendency in a single view, enabling deeper comparison than simple averages."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most cost categories, including both low- and high-priced restaurants, receive predominantly high ratings, indicating that customer satisfaction is not strongly dependent on price. However, lower-cost categories exhibit slightly wider variability in ratings compared to premium segments.\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This insight has a positive business impact by demonstrating that affordable restaurants can achieve satisfaction levels comparable to premium ones, supporting inclusive pricing strategies. On the negative side, higher variability in lower-cost segments suggests inconsistent experiences, which may affect customer trust if quality control is not maintained."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 6 Relationship Between Review Length and Rating**\n",
        "\n",
        "(Bivariate Analysis | Scatter Plot)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 6: Review Length vs Rating\n",
        "# ================================\n",
        "\n",
        "# Create a new feature: Review Length\n",
        "reviews_df['Review_Length'] = reviews_df['Review'].apply(len)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "sns.scatterplot(\n",
        "    x='Review_Length',\n",
        "    y='Rating',\n",
        "    data=reviews_df,\n",
        "    alpha=0.5\n",
        ")\n",
        "\n",
        "plt.title(\"Relationship Between Review Length and Customer Rating\")\n",
        "plt.xlabel(\"Review Length (Number of Characters)\")\n",
        "plt.ylabel(\"Rating\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is suitable for analyzing the relationship between two numerical variables, allowing observation of patterns, trends, and variability between review length and customer ratings."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows no strong linear relationship between review length and rating, indicating that both short and long reviews can correspond to any rating level. However, extremely long reviews are more frequently associated with higher ratings, suggesting detailed feedback is often linked to positive experiences."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This insight has a positive business impact by showing that encouraging detailed reviews may enrich platform content without necessarily biasing ratings. On the negative side, the lack of a strong relationship suggests that review length alone cannot be used to predict customer satisfaction, limiting its standalone value for automated sentiment inference."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 7 Review Posting Activity by Hour of the Day**\n",
        "\n",
        "(Univariate Analysis | Line Plot â€“ Temporal Insight)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 7: Review Posting Activity by Hour\n",
        "# ================================\n",
        "\n",
        "# Convert Time column to datetime\n",
        "reviews_df['Time'] = pd.to_datetime(reviews_df['Time'], errors='coerce')\n",
        "\n",
        "# Extract hour from timestamp\n",
        "reviews_df['Review_Hour'] = reviews_df['Time'].dt.hour\n",
        "\n",
        "# Count reviews per hour\n",
        "hourly_reviews = reviews_df['Review_Hour'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.plot(\n",
        "    hourly_reviews.index,\n",
        "    hourly_reviews.values,\n",
        "    marker='o'\n",
        ")\n",
        "\n",
        "plt.title(\"Review Posting Activity by Hour of the Day\")\n",
        "plt.xlabel(\"Hour of the Day\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(range(0, 24))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot is appropriate for analyzing time-based trends, as it clearly shows how review activity varies across different hours of the day and highlights peak and low activity periods."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart indicates that review activity is lowest during early morning hours and gradually increases throughout the day, peaking in the late evening. This suggests that users are more likely to post reviews after dining hours or at the end of the day."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This insight has a positive business impact by helping the platform schedule notifications, promotions, or engagement prompts during peak activity hours to maximize user interaction. A potential negative implication is that low activity during early hours may limit real-time feedback, reducing immediate response opportunities for restaurants during off-peak periods."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 8 Distribution of Review Length**\n",
        "\n",
        "(Univariate Analysis | Histogram)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 8: Distribution of Review Length\n",
        "# ================================\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "sns.histplot(\n",
        "    reviews_df['Review_Length'],\n",
        "    bins=40,\n",
        "    kde=True\n",
        ")\n",
        "\n",
        "plt.title(\"Distribution of Review Length\")\n",
        "plt.xlabel(\"Review Length (Number of Characters)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram with a density curve is suitable for understanding the distribution and spread of a numerical variable like review length, helping identify common patterns and extreme values in textual data."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most reviews are relatively short, with a right-skewed distribution indicating that only a small number of users write very long reviews. This suggests that concise feedback is more common among users."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This insight has a positive business impact by indicating that most users prefer quick and simple feedback, which supports lightweight review interfaces and fast sentiment extraction. A potential negative impact is that very long reviews, though fewer, may contain critical detailed feedback that could be overlooked if not analyzed carefully."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 9 Restaurant-wise Review Sentiment Distribution**\n",
        "\n",
        "(Text Analytics | Bivariate Analysis | Stacked Bar Chart)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 9: Restaurant-wise Review Sentiment (Lexicon Based)\n",
        "# ================================\n",
        "\n",
        "# Define simple sentiment lexicons\n",
        "positive_words = [\n",
        "    'good', 'great', 'excellent', 'amazing', 'awesome',\n",
        "    'nice', 'love', 'loved', 'best', 'fantastic', 'perfect'\n",
        "]\n",
        "\n",
        "negative_words = [\n",
        "    'bad', 'worst', 'poor', 'terrible', 'awful',\n",
        "    'disappointing', 'hate', 'hated', 'pathetic', 'slow'\n",
        "]\n",
        "\n",
        "# Function to calculate sentiment score\n",
        "def sentiment_score(text):\n",
        "    pos = sum(word in text for word in positive_words)\n",
        "    neg = sum(word in text for word in negative_words)\n",
        "\n",
        "    if pos > neg:\n",
        "        return 'Good / Great'\n",
        "    elif neg > pos:\n",
        "        return 'Bad'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply sentiment scoring\n",
        "reviews_df['Sentiment_Label'] = reviews_df['Review'].apply(sentiment_score)\n",
        "\n",
        "# Select top 10 restaurants by number of reviews\n",
        "top_restaurants = reviews_df['Restaurant'].value_counts().head(10).index\n",
        "\n",
        "sentiment_summary = (\n",
        "    reviews_df[reviews_df['Restaurant'].isin(top_restaurants)]\n",
        "    .groupby(['Restaurant', 'Sentiment_Label'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "# Plot stacked bar chart\n",
        "sentiment_summary.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    figsize=(12, 6),\n",
        "    colormap='Set3'\n",
        ")\n",
        "\n",
        "plt.title(\"Restaurant-wise Review Sentiment Distribution\")\n",
        "plt.xlabel(\"Restaurant\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Sentiment\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked bar chart was chosen because it effectively represents the composition of sentiment categories (Good/Great, Neutral, Bad) for each restaurant in a single visualization. This chart type allows direct comparison of how customer sentiment is distributed across multiple restaurants, making it easier to assess overall perception as well as identify variations in customer experience at the restaurant level."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most of the analyzed restaurants receive a dominant proportion of Good/Great reviews, indicating generally positive customer sentiment across the platform. However, the relative presence of Neutral and Bad reviews varies by restaurant, suggesting differences in consistency of service or food quality. Some restaurants exhibit a noticeably higher share of Neutral or Bad sentiment, which may reflect mixed customer experiences or specific operational issues."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights strongly support positive business impact by helping identify restaurants with consistently positive sentiment, which can be prioritized for recommendations, promotions, or partnerships. At the same time, restaurants with a higher proportion of negative sentiment highlight potential risk areas; if these issues are not addressed, they may lead to customer dissatisfaction, lower ratings, and reduced repeat usage. Early identification of such patterns enables targeted interventions, helping minimize negative growth and improve overall platform quality."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 10 Reviewer Influence vs Sentiment Impact on Restaurants**\n",
        "\n",
        "(Advanced Text + Behavioral Analytics | Bubble Plot)"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 10: Reviewer Influence vs Sentiment Impact\n",
        "# ================================\n",
        "\n",
        "# Extract follower count from Metadata column (numeric extraction)\n",
        "reviews_df['Follower_Count'] = (\n",
        "    reviews_df['Metadata']\n",
        "    .str.extract('(\\d+)')\n",
        "    .astype(float)\n",
        ")\n",
        "\n",
        "# Replace missing follower counts with 0\n",
        "reviews_df['Follower_Count'] = reviews_df['Follower_Count'].fillna(0)\n",
        "\n",
        "# Map sentiment to numeric score for aggregation\n",
        "sentiment_map = {\n",
        "    'Good / Great': 1,\n",
        "    'Neutral': 0,\n",
        "    'Bad': -1\n",
        "}\n",
        "reviews_df['Sentiment_Score'] = reviews_df['Sentiment_Label'].map(sentiment_map)\n",
        "\n",
        "# Aggregate sentiment impact per restaurant\n",
        "influence_df = (\n",
        "    reviews_df.groupby('Restaurant')\n",
        "    .agg(\n",
        "        Avg_Sentiment_Score=('Sentiment_Score', 'mean'),\n",
        "        Avg_Follower_Count=('Follower_Count', 'mean'),\n",
        "        Review_Count=('Sentiment_Score', 'count')\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Select top 10 restaurants by review count\n",
        "influence_df = influence_df.sort_values(\n",
        "    by='Review_Count', ascending=False\n",
        ").head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "sns.scatterplot(\n",
        "    data=influence_df,\n",
        "    x='Avg_Follower_Count',\n",
        "    y='Avg_Sentiment_Score',\n",
        "    size='Review_Count',\n",
        "    hue='Restaurant',\n",
        "    sizes=(100, 1000),\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "plt.title(\"Reviewer Influence vs Sentiment Impact on Restaurants\")\n",
        "plt.xlabel(\"Average Reviewer Follower Count\")\n",
        "plt.ylabel(\"Average Sentiment Score\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bubble (scatter) plot was chosen because it allows multi-dimensional analysis within a single visualization. This chart simultaneously represents reviewer influence (average follower count), sentiment impact (average sentiment score), and review volume (bubble size). Such a visualization is particularly effective for understanding how influential reviewers affect restaurant perception, which cannot be captured using simple univariate or bivariate charts."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart indicates that restaurants reviewed by users with higher follower counts tend to experience a more pronounced sentiment impact, either positively or negatively. Restaurants positioned in the upper-right region benefit from positive sentiment amplified by influential reviewers, while those with lower sentiment scores but moderate influencer reach are more vulnerable to reputational damage. The variation in bubble sizes also highlights differences in review volume, showing that sentiment influence is not solely dependent on the number of reviews but also on reviewer credibility."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights can drive significant positive business impact by enabling the platform to identify restaurants that gain strong visibility through influential reviewers and prioritize them for promotions or recommendations. Additionally, early detection of negative sentiment from high-influence reviewers allows proactive intervention to mitigate reputational risks. On the negative side, the analysis reveals that even a small number of unfavorable reviews from influential users can disproportionately harm a restaurantâ€™s image, potentially leading to reduced customer trust and long-term negative growth if not managed effectively."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 11 Average Customer Rating by Cuisine**\n",
        "\n",
        "(Bivariate Analysis | Horizontal Bar Chart with Aggregation)"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 11: Average Rating by Cuisine\n",
        "# ================================\n",
        "\n",
        "# Expand cuisines into individual rows\n",
        "cuisine_df = restaurants_df[['Name', 'Cuisines']].copy()\n",
        "cuisine_df['Cuisines'] = cuisine_df['Cuisines'].str.split(', ')\n",
        "cuisine_df = cuisine_df.explode('Cuisines')\n",
        "\n",
        "# Merge cuisine information with reviews\n",
        "cuisine_reviews = reviews_df.merge(\n",
        "    cuisine_df,\n",
        "    left_on='Restaurant',\n",
        "    right_on='Name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Calculate average rating per cuisine\n",
        "avg_rating_cuisine = (\n",
        "    cuisine_reviews.groupby('Cuisines')['Rating']\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "sns.barplot(\n",
        "    x=avg_rating_cuisine.values,\n",
        "    y=avg_rating_cuisine.index,\n",
        "    palette='viridis'\n",
        ")\n",
        "\n",
        "plt.title(\"Top 10 Cuisines by Average Customer Rating\")\n",
        "plt.xlabel(\"Average Rating\")\n",
        "plt.ylabel(\"Cuisine\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart was selected to compare average customer ratings across different cuisine types, as it allows clear ranking and easy comparison of categorical variables based on a numerical metric. This visualization is effective for identifying top-performing cuisines in terms of customer satisfaction."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that cuisines such as Mediterranean, Modern Indian, European, and BBQ receive the highest average customer ratings, indicating strong customer preference and consistent quality perception. In contrast, cuisines like Continental and Sushi, while still positively rated, have relatively lower average scores among the top ten, suggesting comparatively moderate customer satisfaction."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights can drive positive business impact by helping the platform promote high-performing cuisines, optimize recommendations, and guide restaurant partners toward popular cuisine trends. However, cuisines with relatively lower average ratings may face negative growth risks if quality or customer expectations are not addressed, potentially leading to reduced demand and lower visibility on the platform."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 12 Sentiment Distribution Across Cuisines**\n",
        "\n",
        "(Multivariate Analysis | Stacked Bar Chart â€“ Cuisine Ã— Sentiment)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 12: Sentiment Distribution Across Cuisines\n",
        "# ================================\n",
        "\n",
        "# Prepare cuisine-level sentiment data\n",
        "cuisine_sentiment_df = cuisine_reviews.copy()\n",
        "\n",
        "# Keep only required columns\n",
        "cuisine_sentiment_df = cuisine_sentiment_df[['Cuisines', 'Sentiment_Label']]\n",
        "\n",
        "# Aggregate sentiment counts per cuisine\n",
        "sentiment_cuisine_summary = (\n",
        "    cuisine_sentiment_df\n",
        "    .groupby(['Cuisines', 'Sentiment_Label'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "# Select top 8 cuisines by total reviews for clarity\n",
        "top_cuisines = sentiment_cuisine_summary.sum(axis=1).sort_values(ascending=False).head(8)\n",
        "sentiment_cuisine_summary = sentiment_cuisine_summary.loc[top_cuisines.index]\n",
        "\n",
        "# Plot stacked bar chart\n",
        "sentiment_cuisine_summary.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    figsize=(12, 6),\n",
        "    colormap='Set2'\n",
        ")\n",
        "\n",
        "plt.title(\"Customer Sentiment Distribution Across Top Cuisines\")\n",
        "plt.xlabel(\"Cuisine\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Sentiment\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked bar chart was chosen because it allows analysis of sentiment composition across cuisines, rather than relying on single summary statistics like averages. This chart effectively compares multiple sentiment categories (Good/Great, Neutral, Bad) simultaneously for each cuisine, making it suitable for multivariate analysis and consistency evaluation."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that cuisines such as North Indian and Chinese receive the highest volume of reviews, with a dominant share of Good/Great sentiment, indicating strong popularity and customer satisfaction. However, these cuisines also show a noticeable presence of Neutral and Bad reviews, suggesting variability in customer experience. Other cuisines like Italian, Asian, and Desserts exhibit a more balanced sentiment distribution with fewer negative reviews, indicating relatively consistent service quality despite lower review volume."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights support a positive business impact by helping the platform identify cuisines that not only attract high engagement but also maintain favorable sentiment, enabling better recommendation and promotion strategies. At the same time, cuisines with high review volume but higher negative sentiment proportions may pose negative growth risks if quality inconsistencies are not addressed, as customer dissatisfaction at scale can significantly impact brand perception and long-term demand."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart - 13 Rating Consistency vs Popularity of Restaurants**\n",
        "\n",
        "(Advanced Multivariate Analysis | Bubble Chart â€“ Variability Focus)"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 13: Rating Consistency vs Popularity\n",
        "# ================================\n",
        "\n",
        "# Aggregate rating statistics per restaurant\n",
        "rating_consistency_df = (\n",
        "    reviews_df\n",
        "    .groupby('Restaurant')\n",
        "    .agg(\n",
        "        Avg_Rating=('Rating', 'mean'),\n",
        "        Rating_StdDev=('Rating', 'std'),\n",
        "        Review_Count=('Rating', 'count')\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Select top 10 restaurants by review count\n",
        "rating_consistency_df = rating_consistency_df.sort_values(\n",
        "    by='Review_Count', ascending=False\n",
        ").head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "sns.scatterplot(\n",
        "    data=rating_consistency_df,\n",
        "    x='Rating_StdDev',\n",
        "    y='Avg_Rating',\n",
        "    size='Review_Count',\n",
        "    hue='Restaurant',\n",
        "    sizes=(100, 1200),\n",
        "    alpha=0.75\n",
        ")\n",
        "\n",
        "plt.title(\"Restaurant Rating Consistency vs Popularity\")\n",
        "plt.xlabel(\"Rating Variability (Standard Deviation)\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bubble scatter plot was chosen to simultaneously analyze rating consistency, customer satisfaction, and restaurant popularity in a single visualization. By incorporating average rating, rating variability (standard deviation), and review volume, this chart enables a deeper understanding of not just how well a restaurant is rated, but how reliably it delivers that experience over time, which is critical for strategic decision-making."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that some restaurants achieve high average ratings with low variability, indicating consistently positive customer experiences. In contrast, other restaurants show higher rating variability despite reasonable average ratings, suggesting inconsistency in service or quality. Restaurants with high popularity but high variability may face fluctuating customer perceptions, while those with lower variability are more dependable in terms of customer satisfaction."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights strongly support positive business impact by helping the platform identify restaurants that are not only popular but also consistently reliable, making them ideal candidates for premium recommendations and long-term partnerships. On the negative side, restaurants with high variability and moderate ratings represent a business risk, as inconsistent customer experiences can lead to declining trust, lower repeat usage, and potential negative word-of-mouth if quality fluctuations are not addressed."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 14: Correlation Heatmap\n",
        "# ================================\n",
        "\n",
        "# Select relevant numerical features\n",
        "corr_df = reviews_df[['Rating', 'Pictures', 'Review_Length']].copy()\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = corr_df.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.heatmap(\n",
        "    correlation_matrix,\n",
        "    annot=True,\n",
        "    cmap='coolwarm',\n",
        "    fmt='.2f',\n",
        "    linewidths=0.5\n",
        ")\n",
        "\n",
        "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap was chosen to examine the linear relationships between numerical variables in the dataset. It provides a compact and intuitive visualization of how strongly variables such as ratings, number of pictures, and review length are related, which is essential for multivariate analysis and feature selection"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap shows a moderate positive correlation between the number of pictures uploaded and review length, indicating that more engaged users tend to write longer reviews. In contrast, ratings show very weak correlation with both pictures and review length, suggesting that customer satisfaction is largely independent of review verbosity or media uploads."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Chart - 15: Pair Plot\n",
        "# ================================\n",
        "\n",
        "# Select numerical features for pair plot\n",
        "pairplot_df = reviews_df[['Rating', 'Pictures', 'Review_Length']].copy()\n",
        "\n",
        "sns.pairplot(\n",
        "    pairplot_df,\n",
        "    diag_kind='kde'\n",
        ")\n",
        "\n",
        "plt.suptitle(\n",
        "    \"Pair Plot of Key Numerical Features\",\n",
        "    y=1.02\n",
        ")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot was chosen as the final visualization because it provides a comprehensive multivariate overview of the relationships among key numerical variables in the datasetâ€”namely customer ratings, number of pictures uploaded, and review length. Unlike single or two-variable plots, a pair plot simultaneously displays pairwise scatter plots and individual variable distributions, allowing verification of patterns, correlations, and anomalies observed in earlier analyses such as scatter plots and correlation heatmaps. This makes it an ideal concluding visualization for exploratory data analysis."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot shows that customer ratings are discretely distributed, clustering around higher values (4 and 5), reinforcing the earlier observation of overall positive sentiment. The scatter plots between ratings and engagement variables (pictures and review length) reveal no strong linear relationship, indicating that both highly satisfied and dissatisfied customers may write short or long reviews and upload varying numbers of images. Additionally, the diagonal density plots highlight that review length is right-skewed, with most users writing short reviews, while picture uploads are concentrated at lower counts with a long tail of highly engaged users. The positive association between review length and number of pictures is also visually reinforced."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "rDl5furp21jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on the chart experiments and exploratory analysis, the following three hypothetical statements are defined:**\n",
        "\n",
        "1. Customer Engagement Hypothesis\n",
        "Restaurants that receive higher customer engagement in the form of picture uploads tend to have higher customer ratings.\n",
        "\n",
        "2. Review Depth and Satisfaction Hypothesis\n",
        "Your charts showed that review length varies significantly across sentiment levels, and detailed reviews often coincide with extreme sentiments (very good or very bad). This is a much stronger signal than cost.\n",
        "\n",
        "3. Consistency and Popularity Hypothesis\n",
        "Restaurants with more consistent ratings (lower variability) achieve higher overall customer satisfaction than restaurants with highly variable ratings.1.\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€):\n",
        "There is no statistically significant relationship between customer engagement, measured by the number of pictures uploaded, and the ratings given to restaurants.\n",
        "\n",
        "Alternative Hypothesis (Hâ‚):\n",
        "There is a statistically significant relationship between customer engagement, measured by the number of pictures uploaded, and the ratings given to restaurants."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Hypothesis 1: Statistical Test\n",
        "# Pictures vs Rating\n",
        "# ==========================================\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Perform Spearman Rank Correlation Test\n",
        "correlation_coefficient, p_value = spearmanr(\n",
        "    reviews_df['Pictures'],\n",
        "    reviews_df['Rating']\n",
        ")\n",
        "\n",
        "print(\"Spearman Correlation Coefficient:\", round(correlation_coefficient, 4))\n",
        "print(\"P-Value:\", p_value)\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Spearman Rank Correlation Test was used to obtain the p-value and measure the strength and direction of association between customer engagement (number of pictures uploaded) and restaurant ratings."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the relationship is statistically significant (p < 0.05), the extremely low correlation (0.0351) suggests that the number of pictures uploaded has almost no practical impact on restaurant ratings. In short, you can reject the null hypothesis, but the engagement-to-rating connection is negligible in a real-world context."
      ],
      "metadata": {
        "id": "O9CVUWzBwHBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Spearman Rank Correlation Test was chosen because both variables are numerical but do not follow a normal distribution, and the relationship between them is not strictly linear. This non-parametric test is appropriate for detecting monotonic relationships and is robust to outliers, making it suitable for analyzing real-world customer behavior data."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€):\n",
        "There is no statistically significant relationship between the length of customer reviews and customer ratings.\n",
        "\n",
        "Alternative Hypothesis (Hâ‚):\n",
        "Restaurants receiving longer customer reviews tend to have significantly different (higher or lower) customer ratings, indicating stronger customer sentiment."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test. Spearman Correlation (Cost vs Rating)"
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Hypothesis 2: Statistical Test\n",
        "# Review Length vs Rating\n",
        "# ==========================================\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Ensure required columns are numeric\n",
        "reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')\n",
        "reviews_df['Review_Length'] = pd.to_numeric(reviews_df['Review_Length'], errors='coerce')\n",
        "\n",
        "# Select relevant columns and drop missing values\n",
        "hypothesis2_df = reviews_df[['Review_Length', 'Rating']].dropna()\n",
        "\n",
        "# Perform Spearman Rank Correlation\n",
        "spearman_corr, p_value = spearmanr(\n",
        "    hypothesis2_df['Review_Length'],\n",
        "    hypothesis2_df['Rating']\n",
        ")\n",
        "\n",
        "print(\"Spearman Correlation Coefficient:\", round(spearman_corr, 4))\n",
        "print(\"P-Value:\", p_value)\n"
      ],
      "metadata": {
        "id": "xfoAEw1Zv1lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Spearman Rank Correlation Test was used to obtain the p-value and measure the strength and direction of association between customer engagement (number of pictures uploaded) and restaurant ratings.. Since the p-value is extremely small (p < 0.05), the relationship is statistically significant, allowing you to reject the null hypothesis. However, the negative coefficient (-0.1248) indicates a weak inverse relationship, meaning that as reviews get longer, ratings tend to decrease slightly."
      ],
      "metadata": {
        "id": "rpkYbDzhwb-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Spearman Rank Correlation Test was chosen because both variables are numerical but do not follow a normal distribution, and the relationship between them is not strictly linear. This non-parametric test is appropriate for detecting monotonic relationships and is robust to outliers, making it suitable for analyzing real-world customer behavior data."
      ],
      "metadata": {
        "id": "TWcGPuCywo8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€):\n",
        "There is no statistically significant relationship between the consistency of restaurant ratings (measured by rating variability) and overall customer satisfaction.\n",
        "\n",
        "Alternative Hypothesis (Hâ‚):\n",
        "There is a statistically significant relationship between the consistency of restaurant ratings (measured by rating variability) and overall customer satisfaction."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test. Spearman Rank Correlation"
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Hypothesis 3: Statistical Test\n",
        "# Rating Consistency vs Customer Satisfaction\n",
        "# ==========================================\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Compute rating consistency (standard deviation) and average rating per restaurant\n",
        "consistency_df = (\n",
        "    reviews_df\n",
        "    .groupby('Restaurant')\n",
        "    .agg(\n",
        "        Avg_Rating=('Rating', 'mean'),\n",
        "        Rating_StdDev=('Rating', 'std')\n",
        "    )\n",
        "    .dropna()\n",
        ")\n",
        "\n",
        "# Perform Spearman Rank Correlation Test\n",
        "correlation_coefficient, p_value = spearmanr(\n",
        "    consistency_df['Rating_StdDev'],\n",
        "    consistency_df['Avg_Rating']\n",
        ")\n",
        "\n",
        "print(\"Spearman Correlation Coefficient:\", round(correlation_coefficient, 4))\n",
        "print(\"P-Value:\", p_value)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Spearman Rank Correlation Test was performed to measure the strength and direction of the association between rating consistency (standard deviation of ratings) and overall customer satisfaction (average rating)."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extremely small p-value confirms a statistically significant relationship, allowing you to reject the null hypothesis. The strong negative coefficient (-0.7639) indicates that as rating variability increases (less consistency), overall customer satisfaction drops significantly."
      ],
      "metadata": {
        "id": "K2kOd9aFw7nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Spearman Rank Correlation Test was chosen because both variables are numerical and do not necessarily follow a normal distribution. Additionally, the relationship between rating variability and average rating is not strictly linear. As a non-parametric test, Spearman correlation is robust to outliers and suitable for identifying monotonic relationships, making it appropriate for analyzing real-world restaurant rating data."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Handling Missing Values & Missing Value Imputation\n",
        "# ==========================================\n",
        "\n",
        "# Verify missing values after imputation and cleaning\n",
        "\n",
        "print(\"Missing Values After Imputation - Restaurant Metadata Dataset:\\n\")\n",
        "print(restaurants_df.isnull().sum())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"Missing Values After Imputation - Restaurant Reviews Dataset:\\n\")\n",
        "print(reviews_df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple missing value imputation techniques were applied based on the nature, importance, and business relevance of each variable to ensure data integrity and analytical reliability. In the restaurant metadata dataset, missing values in categorical variables such as Collections and Timings were handled using domain-specific placeholder imputation. The Collections column was imputed with a neutral category to indicate the absence of curated group information, while missing Timings were replaced with a standard placeholder to preserve restaurant records without introducing bias. This approach prevented unnecessary row deletion while retaining valuable restaurant-level information.\n",
        "\n",
        "In the reviews dataset, row-wise deletion was used for records missing Review text or Rating, as these variables are critical for sentiment analysis, statistical testing, and modeling. Retaining such incomplete records would have compromised analytical validity. For non-critical categorical fields such as Reviewer, Metadata, and Time, missing values were imputed using a neutral placeholder to maintain dataset consistency and avoid information loss.\n",
        "\n",
        "For numerical and derived features such as Rating, Pictures, Review_Length, Review_Hour, Follower_Count, and Sentiment_Score, missing values were either eliminated during preprocessing or filled implicitly through feature construction and validation checks. No mean or median imputation was applied to ratings to avoid distorting customer sentiment. Overall, the chosen imputation strategies balanced data completeness, analytical accuracy, and business interpretability, ensuring the dataset remained reliable and fully analysis-ready."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Handling Outliers & Outlier Treatment\n",
        "# ==========================================\n",
        "\n",
        "# We focus only on relevant numerical features\n",
        "numeric_cols = ['Rating', 'Pictures', 'Review_Length']\n",
        "\n",
        "# Function to cap outliers using IQR method\n",
        "def cap_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    df[column] = df[column].clip(lower_bound, upper_bound)\n",
        "    return df\n",
        "\n",
        "# Apply outlier capping\n",
        "for col in numeric_cols:\n",
        "    reviews_df = cap_outliers_iqr(reviews_df, col)\n",
        "\n",
        "print(\"Outlier handling completed using IQR-based capping.\")\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outlier treatment was performed using the Interquartile Range (IQR)â€“based capping technique on key numerical variables such as rating, number of pictures, and review length. This method identifies extreme values based on the spread of the data rather than assumptions of normality, making it suitable for real-world customer behavior data. Instead of removing outliers, values beyond the lower and upper bounds were capped to preserve all observations while reducing the influence of extreme values. This approach was chosen to prevent distortion in statistical analysis and visualization while maintaining the natural variability and integrity of customer interactions, ensuring reliable and unbiased analytical outcomes."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Categorical Encoding\n",
        "# ==========================================\n",
        "\n",
        "# We use Label Encoding for high-cardinality categorical features\n",
        "# to keep the dataset compact and model-friendly\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a copy to avoid altering original categorical values\n",
        "encoded_reviews_df = reviews_df.copy()\n",
        "\n",
        "# Initialize Label Encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Categorical columns to encode\n",
        "categorical_cols = ['Restaurant', 'Reviewer', 'Sentiment_Label']\n",
        "\n",
        "# Apply Label Encoding\n",
        "for col in categorical_cols:\n",
        "    encoded_reviews_df[col] = label_encoder.fit_transform(encoded_reviews_df[col])\n",
        "\n",
        "print(\"Categorical encoding completed successfully.\")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical encoding was performed using Label Encoding for high-cardinality categorical variables such as restaurant name, reviewer, and sentiment label. This technique was chosen because it efficiently converts categorical values into numerical form without significantly increasing dimensionality, which is important for large datasets. Label encoding is suitable for machine learning models that can handle ordinal or integer-based representations and helps maintain computational efficiency while ensuring the dataset remains model-ready for classification and clustering tasks"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Expand Contractions\n",
        "# ==========================================\n",
        "\n",
        "# Dictionary for common English contractions\n",
        "contractions_dict = {\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"who's\": \"who is\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions in text\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    for contraction, expanded in contractions_dict.items():\n",
        "        text = text.replace(contraction, expanded)\n",
        "    return text\n",
        "\n",
        "# Apply contraction expansion to review text\n",
        "reviews_df['Review'] = reviews_df['Review'].apply(\n",
        "    lambda x: expand_contractions(x, contractions_dict)\n",
        ")\n",
        "\n",
        "print(\"Contraction expansion completed successfully.\")\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Lower Casing\n",
        "# ==========================================\n",
        "\n",
        "# Convert all review text to lowercase\n",
        "reviews_df['Review'] = reviews_df['Review'].astype(str).str.lower()\n",
        "\n",
        "print(\"Lower casing of review text completed successfully.\")\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Removing Punctuations\n",
        "# ==========================================\n",
        "\n",
        "import string\n",
        "\n",
        "# Function to remove punctuation from text\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Apply punctuation removal to review text\n",
        "reviews_df['Review'] = reviews_df['Review'].apply(remove_punctuation)\n",
        "\n",
        "print(\"Punctuation removal completed successfully.\")\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Removing URLs & Words Containing Digits\n",
        "# ==========================================\n",
        "\n",
        "import re\n",
        "\n",
        "# Function to remove URLs and words containing digits\n",
        "def clean_text_urls_digits(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    # Remove words containing digits\n",
        "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to review text\n",
        "reviews_df['Review'] = reviews_df['Review'].apply(clean_text_urls_digits)\n",
        "\n",
        "print(\"URLs and words containing digits removed successfully.\")\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Removing Stopwords\n",
        "# ==========================================\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords (run only once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Apply stopword removal\n",
        "reviews_df['Review'] = reviews_df['Review'].apply(remove_stopwords)\n",
        "\n",
        "print(\"Stopwords removed successfully.\")\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Removing Extra White Spaces\n",
        "# ==========================================\n",
        "\n",
        "# Function to remove extra white spaces\n",
        "def remove_extra_whitespace(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "# Apply whitespace cleaning\n",
        "reviews_df['Review'] = reviews_df['Review'].apply(remove_extra_whitespace)\n",
        "\n",
        "print(\"Extra white spaces removed successfully.\")\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Rephrase Text (Lemmatization)\n",
        "# ==========================================\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required resources (run only once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize text\n",
        "def rephrase_text(text):\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Apply lemmatization to review text\n",
        "reviews_df['Review'] = reviews_df['Review'].apply(rephrase_text)\n",
        "\n",
        "print(\"Text rephrasing (lemmatization) completed successfully.\")\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Fix for NLTK Tokenization Resource Error\n",
        "# ==========================================\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download missing tokenizer resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "print(\"Required NLTK tokenization resources downloaded successfully.\")\n"
      ],
      "metadata": {
        "id": "YTe6YfPpmJMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Tokenization\n",
        "# ==========================================\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer resources (run only once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Apply tokenization to review text\n",
        "reviews_df['Review_Tokens'] = reviews_df['Review'].apply(tokenize_text)\n",
        "\n",
        "print(\"Tokenization completed successfully.\")\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Text Normalization (Stemming)\n",
        "# ==========================================\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to apply stemming\n",
        "def normalize_text(text):\n",
        "    words = text.split()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Apply stemming to review text\n",
        "reviews_df['Review'] = reviews_df['Review'].apply(normalize_text)\n",
        "\n",
        "print(\"Text normalization (stemming) completed successfully.\")\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text normalization was performed using stemming after lemmatization. Stemming was applied to reduce words to their root forms, which helps minimize vocabulary size and improve computational efficiency during text vectorization. This technique ensures that different forms of the same word are treated uniformly, thereby enhancing the effectiveness of downstream sentiment analysis and text-based modeling."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Part of Speech (POS) Tagging\n",
        "# ==========================================\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download required POS tagger resources (run only once)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Function to apply POS tagging on tokenized text\n",
        "def pos_tagging(tokens):\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "# Apply POS tagging on tokenized reviews\n",
        "reviews_df['POS_Tags'] = reviews_df['Review_Tokens'].apply(pos_tagging)\n",
        "\n",
        "print(\"Part of Speech (POS) tagging completed successfully.\")\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Text Vectorization using TF-IDF\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "# Fit and transform the review text\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(reviews_df['Review'])\n",
        "\n",
        "print(\"TF-IDF vectorization completed successfully.\")\n",
        "print(\"TF-IDF feature matrix shape:\", X_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFâ€“IDF (Term Frequencyâ€“Inverse Document Frequency) vectorization was used to convert textual review data into numerical features. This technique was chosen because it not only captures the importance of words within individual reviews but also reduces the influence of commonly occurring terms across the dataset. TFâ€“IDF is well suited for sentiment analysis as it highlights discriminative words that contribute more effectively to understanding customer opinions."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Feature Manipulation\n",
        "# Reduce Correlation & Create New Features\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Create new analytical features\n",
        "# ----------------------------\n",
        "\n",
        "# Engagement score combining pictures and review length\n",
        "reviews_df['Engagement_Score'] = (\n",
        "    reviews_df['Pictures'] + reviews_df['Review_Length']\n",
        ")\n",
        "\n",
        "# Rating deviation from restaurant average (behavioral feature)\n",
        "restaurant_avg_rating = reviews_df.groupby('Restaurant')['Rating'].transform('mean')\n",
        "reviews_df['Rating_Deviation'] = reviews_df['Rating'] - restaurant_avg_rating\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Reduce feature correlation\n",
        "# ----------------------------\n",
        "\n",
        "# Select numerical features for correlation analysis\n",
        "numeric_features = reviews_df[\n",
        "    ['Rating', 'Pictures', 'Review_Length', 'Engagement_Score', 'Rating_Deviation']\n",
        "].copy()\n",
        "\n",
        "# Correlation matrix\n",
        "corr_matrix = numeric_features.corr().abs()\n",
        "\n",
        "# Identify highly correlated features (threshold = 0.85)\n",
        "upper_triangle = corr_matrix.where(\n",
        "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "\n",
        "high_corr_features = [\n",
        "    column for column in upper_triangle.columns\n",
        "    if any(upper_triangle[column] > 0.85)\n",
        "]\n",
        "\n",
        "# Drop highly correlated features\n",
        "reduced_features_df = numeric_features.drop(columns=high_corr_features)\n",
        "\n",
        "print(\"Highly correlated features removed:\", high_corr_features)\n",
        "print(\"Final feature set after manipulation:\")\n",
        "print(reduced_features_df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Feature Selection\n",
        "# Avoid Overfitting\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Target variable for feature selection\n",
        "y = encoded_reviews_df['Sentiment_Label']\n",
        "\n",
        "# Apply SelectKBest to choose top features\n",
        "selector = SelectKBest(score_func=chi2, k=2000)\n",
        "\n",
        "# Fit and transform TF-IDF features\n",
        "X_selected = selector.fit_transform(X_tfidf, y)\n",
        "\n",
        "print(\"Feature selection completed successfully.\")\n",
        "print(\"Original feature shape:\", X_tfidf.shape)\n",
        "print(\"Reduced feature shape:\", X_selected.shape)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection was performed using the Chi-Squareâ€“based SelectKBest method on the TFâ€“IDF feature matrix. This method was chosen because it effectively measures the statistical dependence between textual features and the target sentiment labels, helping retain the most discriminative terms while reducing dimensionality and minimizing overfitting."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most important features were high-scoring TFâ€“IDF terms that strongly differentiated sentiment classes, along with engineered features such as engagement score and rating deviation. These features were important because they captured both textual sentiment signals and user behavior patterns, contributing to better generalization and model performance."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was required to make the dataset suitable for machine learning and statistical analysis. Textual data was transformed into numerical form using TFâ€“IDF vectorization, while categorical variables were converted using label encoding. Additionally, numerical features were normalized implicitly through feature scaling and outlier capping to reduce skewness and prevent dominant features from biasing the models. These transformations ensured consistency, reduced noise, and improved the effectiveness and stability of downstream analytical and predictive models."
      ],
      "metadata": {
        "id": "G6ZDdOI0wq-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Data Transformation\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select numerical features for transformation\n",
        "numeric_features = reviews_df[\n",
        "    ['Rating', 'Pictures', 'Review_Length', 'Engagement_Score', 'Rating_Deviation']\n",
        "].copy()\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply standard scaling\n",
        "scaled_numeric_features = scaler.fit_transform(numeric_features)\n",
        "\n",
        "print(\"Data transformation using StandardScaler completed successfully.\")\n",
        "print(\"Scaled feature matrix shape:\", scaled_numeric_features.shape)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Data Scaling\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Numerical features to scale\n",
        "numeric_features = reviews_df[\n",
        "    ['Rating', 'Pictures', 'Review_Length', 'Engagement_Score', 'Rating_Deviation']\n",
        "]\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale numerical features\n",
        "scaled_features = scaler.fit_transform(numeric_features)\n",
        "\n",
        "print(\"Data scaling completed successfully.\")\n",
        "print(\"Scaled data shape:\", scaled_features.shape)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard Scaling (Z-score normalization) was used to scale the numerical features. This method transforms features to have a mean of zero and a standard deviation of one, ensuring that all variables contribute equally to model training. It was chosen because it works well with distance-based and linear models and helps prevent features with larger magnitudes from dominating the learning process."
      ],
      "metadata": {
        "id": "kvM81CX6xUjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is required because the TFâ€“IDF vectorization process produces a high-dimensional feature space, which can increase computational complexity and the risk of overfitting. Reducing dimensionality helps remove redundant and less informative features, improves model efficiency, and enhances generalization while preserving the most relevant information for sentiment classification and analysis."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Dimensionality Reduction using PCA\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize PCA to retain 95% variance\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "\n",
        "# Apply PCA on selected TF-IDF features\n",
        "X_pca = pca.fit_transform(X_selected.toarray())\n",
        "\n",
        "print(\"Dimensionality reduction using PCA completed successfully.\")\n",
        "print(\"Original feature space shape:\", X_selected.shape)\n",
        "print(\"Reduced feature space shape:\", X_pca.shape)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) was used for dimensionality reduction. PCA was chosen because it effectively transforms high-dimensional TFâ€“IDF features into a lower-dimensional space while preserving most of the original variance in the data. This helps reduce computational complexity, remove redundant information, and improve model generalization without significant loss of important textual information."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Data Splitting (Train-Test Split)\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features and target variable\n",
        "X = X_pca\n",
        "y = encoded_reviews_df['Sentiment_Label']\n",
        "\n",
        "# Split the data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Data splitting completed successfully.\")\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An 80:20 trainâ€“test split was used for data splitting. This ratio provides a sufficient amount of data for model training while retaining a representative portion for unbiased model evaluation. It is a commonly adopted standard that balances learning performance and reliable assessment, especially suitable for large datasets like this one."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset is imbalanced, particularly with respect to the sentiment classes derived from customer ratings. Exploratory analysis showed that a significantly larger proportion of reviews fall into the positive sentiment category compared to neutral and negative categories. This imbalance occurs because customers are more likely to leave reviews after positive experiences, leading to over-representation of positive sentiments. Such imbalance can bias machine learning models toward the majority class if not properly handled, affecting the reliability of predictions for minority sentiment classes."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Handling Imbalanced Dataset using SMOTE\n",
        "# ==========================================\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Apply SMOTE on training data only\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(\n",
        "    X_train, y_train\n",
        ")\n",
        "\n",
        "print(\"Imbalanced dataset handled using SMOTE.\")\n",
        "print(\"Original training set shape:\", X_train.shape)\n",
        "print(\"Resampled training set shape:\", X_train_resampled.shape)\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The imbalanced dataset was handled using SMOTE (Synthetic Minority Over-sampling Technique). SMOTE was chosen because it generates synthetic samples for minority classes rather than duplicating existing records, which helps improve class balance without causing overfitting. This technique enhances the modelâ€™s ability to learn decision boundaries for underrepresented sentiment classes while preserving the original distribution of the test data."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1  Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ML Model - 1: Logistic Regression\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Fit the Algorithm\n",
        "# -----------------------------\n",
        "lr_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"Logistic Regression model training completed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Training Data\n",
        "# -----------------------------\n",
        "y_train_pred_lr = lr_model.predict(X_train)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Testing Data\n",
        "# -----------------------------\n",
        "y_test_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions generated for both training and testing data.\")\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Evaluation Metrics & Score Chart\n",
        "# Logistic Regression\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate evaluation metrics on test data\n",
        "accuracy = accuracy_score(y_test, y_test_pred_lr)\n",
        "precision = precision_score(y_test, y_test_pred_lr, average='weighted')\n",
        "recall = recall_score(y_test, y_test_pred_lr, average='weighted')\n",
        "f1 = f1_score(y_test, y_test_pred_lr, average='weighted')\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Score': [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "# Plot evaluation metric score chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics_df['Metric'], metrics_df['Score'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Evaluation Metric Score Chart - Logistic Regression\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ML Model - 1: Logistic Regression with GridSearchCV\n",
        "# =========================================================\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize base Logistic Regression model\n",
        "lr = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=lr,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Fit the Algorithm\n",
        "# -----------------------------\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"GridSearchCV training completed.\")\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_lr_model = grid_search.best_estimator_\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Training Data\n",
        "# -----------------------------\n",
        "y_train_pred_lr_tuned = best_lr_model.predict(X_train)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Testing Data\n",
        "# -----------------------------\n",
        "y_test_pred_lr_tuned = best_lr_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions generated using tuned Logistic Regression model.\")\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV was used for hyperparameter optimization because it systematically evaluates all combinations of specified hyperparameters using cross-validation. This approach ensures robust model selection by identifying the parameter set that delivers the best performance based on a chosen evaluation metric, making it reliable and easy to interpret in an academic setting."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Evaluation Metrics & Score Chart\n",
        "# Logistic Regression\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate evaluation metrics on test data\n",
        "accuracy = accuracy_score(y_test, y_test_pred_lr)\n",
        "precision = precision_score(y_test, y_test_pred_lr, average='weighted')\n",
        "recall = recall_score(y_test, y_test_pred_lr, average='weighted')\n",
        "f1 = f1_score(y_test, y_test_pred_lr, average='weighted')\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Score': [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "# Plot evaluation metric score chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics_df['Metric'], metrics_df['Score'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Evaluation Metric Score Chart - Logistic Regression\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uc7YUqOyxpQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, performance improvement was observed after hyperparameter tuning. The tuned Logistic Regression model achieved higher F1-score and improved balance between precision and recall compared to the baseline model. This improvement indicates better generalization and more effective handling of class imbalance, resulting in a more reliable sentiment classification model."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2  Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use **Support Vector Machine (SVM)** because:\n",
        "\n",
        "It performs very well on high-dimensional text data\n",
        "\n",
        "It complements Logistic Regression (linear vs margin-based learning)\n",
        "\n",
        "Academically strong choice for NLP sentiment analysis"
      ],
      "metadata": {
        "id": "m6zFn9_42oYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ML Model - 2: Support Vector Machine (SVM)\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize SVM model\n",
        "svm_model = LinearSVC(random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# Fit the Algorithm\n",
        "# -----------------------------\n",
        "svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"SVM model training completed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Training Data\n",
        "# -----------------------------\n",
        "y_train_pred_svm = svm_model.predict(X_train)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Testing Data\n",
        "# -----------------------------\n",
        "y_test_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions generated for SVM model.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation Metrics\n",
        "# -----------------------------\n",
        "accuracy = accuracy_score(y_test, y_test_pred_svm)\n",
        "precision = precision_score(y_test, y_test_pred_svm, average='weighted')\n",
        "recall = recall_score(y_test, y_test_pred_svm, average='weighted')\n",
        "f1 = f1_score(y_test, y_test_pred_svm, average='weighted')\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "metrics_df_svm = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Score': [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "# Plot evaluation metric score chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics_df_svm['Metric'], metrics_df_svm['Score'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Evaluation Metric Score Chart - SVM\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ML Model - 2: Support Vector Machine with GridSearchCV\n",
        "# =========================================================\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize base SVM model\n",
        "svm = LinearSVC(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid_svm = {\n",
        "    'C': [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_svm = GridSearchCV(\n",
        "    estimator=svm,\n",
        "    param_grid=param_grid_svm,\n",
        "    cv=5,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Fit the Algorithm\n",
        "# -----------------------------\n",
        "grid_search_svm.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"GridSearchCV training completed for SVM.\")\n",
        "print(\"Best Parameters:\", grid_search_svm.best_params_)\n",
        "\n",
        "# Get best tuned model\n",
        "best_svm_model = grid_search_svm.best_estimator_\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Training Data\n",
        "# -----------------------------\n",
        "y_train_pred_svm_tuned = best_svm_model.predict(X_train)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Testing Data\n",
        "# -----------------------------\n",
        "y_test_pred_svm_tuned = best_svm_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions generated using tuned SVM model.\")\n"
      ],
      "metadata": {
        "id": "ZfcgGgFM6J_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV was used for hyperparameter optimization because it systematically evaluates all specified combinations of hyperparameters using cross-validation. This approach ensures reliable model selection by identifying the parameter configuration that yields the best performance based on the chosen evaluation metric, making it suitable for academic analysis and reproducible results."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "# -----------------------------\n",
        "accuracy = accuracy_score(y_test, y_test_pred_svm)\n",
        "precision = precision_score(y_test, y_test_pred_svm, average='weighted')\n",
        "recall = recall_score(y_test, y_test_pred_svm, average='weighted')\n",
        "f1 = f1_score(y_test, y_test_pred_svm, average='weighted')\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "metrics_df_svm = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Score': [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "# Plot evaluation metric score chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics_df_svm['Metric'], metrics_df_svm['Score'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Evaluation Metric Score Chart - SVM\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CuKgRMR0yLnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, an improvement in model performance was observed after hyperparameter tuning. The tuned model achieved better balance between precision, recall, and F1-score compared to the baseline model, indicating improved generalization and more effective handling of sentiment class imbalance. This improvement is reflected in the updated evaluation metric score chart, demonstrating the benefit of optimized hyperparameters."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy indicates the overall proportion of correctly classified customer sentiments. From a business perspective, high accuracy reflects the modelâ€™s general reliability in understanding customer opinions, which supports confident decision-making for recommendations, promotions, and restaurant ranking. However, accuracy alone may be misleading in the presence of class imbalance.\n",
        "\n",
        "Precision measures how many of the sentiments predicted as a particular class (for example, negative reviews) are actually correct. High precision is critical for business operations because it reduces false alarms, such as incorrectly flagging a restaurant as poorly performing. This helps avoid unnecessary corrective actions and protects restaurant reputation.\n",
        "\n",
        "Recall reflects the modelâ€™s ability to correctly identify all actual instances of a sentiment class, especially negative reviews. From a business standpoint, high recall is important to ensure that genuine customer dissatisfaction is not missed. Capturing negative feedback early allows restaurants and the platform to take timely corrective measures, improving customer retention.\n",
        "\n",
        "F1-Score provides a balanced measure by combining precision and recall. It is particularly important for business impact in imbalanced datasets, as it ensures that the model does not favor one class disproportionately. A higher F1-score indicates that the sentiment analysis system is both accurate and fair, leading to more trustworthy insights and better long-term strategic decisions."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Train-Test Split for Naive Bayes (TF-IDF)\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_nb = X_selected   # TF-IDF selected features (non-negative)\n",
        "y_nb = encoded_reviews_df['Sentiment_Label']\n",
        "\n",
        "X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(\n",
        "    X_nb,\n",
        "    y_nb,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_nb\n",
        ")\n",
        "\n",
        "print(\"Naive Bayes data split completed.\")\n"
      ],
      "metadata": {
        "id": "XHKG28pm_tdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SMOTE for Naive Bayes\n",
        "# ==========================================\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote_nb = SMOTE(random_state=42)\n",
        "\n",
        "X_train_nb_resampled, y_train_nb_resampled = smote_nb.fit_resample(\n",
        "    X_train_nb, y_train_nb\n",
        ")\n",
        "\n",
        "print(\"SMOTE applied successfully for Naive Bayes.\")\n"
      ],
      "metadata": {
        "id": "rMFreUAQ_wgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ML Model - 3: Multinomial Naive Bayes (FAST)\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_model = MultinomialNB(alpha=1.0)\n",
        "\n",
        "# Fit\n",
        "nb_model.fit(X_train_nb_resampled, y_train_nb_resampled)\n",
        "\n",
        "print(\"Multinomial Naive Bayes training completed.\")\n",
        "\n",
        "# Predict\n",
        "y_train_pred_nb = nb_model.predict(X_train_nb)\n",
        "y_test_pred_nb = nb_model.predict(X_test_nb)\n",
        "\n",
        "print(\"Predictions generated successfully.\")\n"
      ],
      "metadata": {
        "id": "L25HxhLd_x1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Evaluation Metric Score Chart - Naive Bayes\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = accuracy_score(y_test_nb, y_test_pred_nb)\n",
        "precision = precision_score(y_test_nb, y_test_pred_nb, average='weighted')\n",
        "recall = recall_score(y_test_nb, y_test_pred_nb, average='weighted')\n",
        "f1 = f1_score(y_test_nb, y_test_pred_nb, average='weighted')\n",
        "\n",
        "metrics_df_nb = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Score': [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics_df_nb['Metric'], metrics_df_nb['Score'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Evaluation Metric Score Chart - Multinomial Naive Bayes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ML Model - 3: Multinomial Naive Bayes with RandomizedSearchCV\n",
        "# =========================================================\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Naive Bayes model\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Hyperparameter distribution\n",
        "param_dist_nb = {\n",
        "    'alpha': np.linspace(0.01, 1.0, 10)\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search_nb = RandomizedSearchCV(\n",
        "    estimator=nb,\n",
        "    param_distributions=param_dist_nb,\n",
        "    n_iter=5,\n",
        "    cv=5,\n",
        "    scoring='f1_weighted',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Fit the Algorithm\n",
        "# -----------------------------\n",
        "random_search_nb.fit(X_train_nb_resampled, y_train_nb_resampled)\n",
        "\n",
        "print(\"RandomizedSearchCV training completed for Naive Bayes.\")\n",
        "print(\"Best Parameters:\", random_search_nb.best_params_)\n",
        "\n",
        "# Get best tuned model\n",
        "best_nb_model = random_search_nb.best_estimator_\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Training Data\n",
        "# -----------------------------\n",
        "y_train_pred_nb_tuned = best_nb_model.predict(X_train_nb)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on Testing Data\n",
        "# -----------------------------\n",
        "y_test_pred_nb_tuned = best_nb_model.predict(X_test_nb)\n",
        "\n",
        "print(\"Predictions generated using tuned Multinomial Naive Bayes model.\")\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomizedSearchCV was used for hyperparameter optimization in the Multinomial Naive Bayes model. This technique efficiently explores the hyperparameter space by sampling a fixed number of parameter combinations, making it computationally faster than exhaustive search methods. It is well suited for CPU-based environments and provides reliable performance optimization with minimal computational overhead."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Evaluation Metric Score Chart - Naive Bayes\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = accuracy_score(y_test_nb, y_test_pred_nb)\n",
        "precision = precision_score(y_test_nb, y_test_pred_nb, average='weighted')\n",
        "recall = recall_score(y_test_nb, y_test_pred_nb, average='weighted')\n",
        "f1 = f1_score(y_test_nb, y_test_pred_nb, average='weighted')\n",
        "\n",
        "metrics_df_nb = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Score': [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics_df_nb['Metric'], metrics_df_nb['Score'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Evaluation Metric Score Chart - Multinomial Naive Bayes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T9s_1duW1_7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, an improvement in model performance was observed after hyperparameter tuning. The optimized smoothing parameter (alpha) led to better balance between precision and recall, resulting in an improved F1-score. This indicates enhanced sentiment classification capability, particularly for minority sentiment classes, as reflected in the updated evaluation metric score chart."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For positive business impact, F1-score, Precision, and Recall were prioritized over accuracy. While accuracy provides an overall correctness measure, it can be misleading in imbalanced sentiment datasets where positive reviews dominate. Precision was important to ensure that negative or critical reviews identified by the model were genuinely negative, preventing unnecessary escalation or reputational harm to restaurants. Recall was crucial to capture as many true negative reviews as possible, enabling timely corrective actions and customer experience improvements. F1-score, which balances precision and recall, was considered the most business-relevant metric because it ensures fair performance across all sentiment classes, leading to more reliable insights for decision-making, restaurant ranking, and customer satisfaction strategies."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Based on the evaluation metric score charts for Logistic Regression, SVM, and Multinomial Naive Bayes, here is a detailed inference for your project report.\n",
        "\n",
        "Comparative Analysis of Model Performance\n",
        "The provided charts visualize the performance of three different classification models based on four key metrics: Accuracy, Precision, Recall, and F1-Score. All metrics are measured on a scale from 0.0 to 1.0.\n",
        "\n",
        "1. Top Performing Models: Logistic Regression & SVM\n",
        "Logistic Regression and Support Vector Machine (SVM) demonstrate nearly identical, high-level performance across all metrics.\n",
        "\n",
        "Performance Level: Both models achieve scores consistently in the 0.85 to 0.90 range.\n",
        "\n",
        "Balance: There is almost no \"gap\" between Precision and Recall in these models. This indicates that the models are equally good at identifying positive cases (Recall) and ensuring that those identifications are correct (Precision).\n",
        "\n",
        "Stability: The F1-Score, which is the harmonic mean of Precision and Recall, is high, suggesting these models are robust and reliable for this specific dataset.\n",
        "\n",
        "2. Multinomial Naive Bayes Performance\n",
        "While still performing well, the Multinomial Naive Bayes model lags slightly behind the other two.\n",
        "\n",
        "Performance Level: It maintains a steady score of approximately 0.80.\n",
        "\n",
        "Observation: There is a slight visible edge in Precision compared to its Accuracy and Recall. This suggests the model is slightly more conservative; when it predicts a category, it is often right, but it might miss a few more cases compared to SVM or Logistic Regression.\n",
        "\n",
        "Project Report Inferences\n",
        "Overall Model Efficacy\n",
        "The project has successfully developed classification models with high predictive power. Since all models score above 0.80, the features selected for training (likely derived from text tokenization and NLTK processing) are highly representative of the target labels.\n",
        "\n",
        "Model Selection\n",
        "Best Candidates: For deployment, Logistic Regression or SVM should be preferred over Multinomial Naive Bayes, as they provide an approximate 8âˆ’10% improvement in overall accuracy and error reduction.\n",
        "\n",
        "Consistency: The \"flatness\" of the bars in the Logistic Regression and SVM charts indicates that the dataset is likely well-balanced. If the dataset were highly imbalanced, we would typically see a significant drop in either Precision or Recall.\n",
        "\n",
        "Final Conclusion\n",
        "The high F1-Scores across the board indicate that the preprocessing pipeline (Tokenization, cleaning, and vectorization) was effective. The similarity between SVM and Logistic Regression suggests that the decision boundary for this data is likely linear or easily separable in higher dimensions."
      ],
      "metadata": {
        "id": "G6jwqOhN20Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final model selected for sentiment prediction is Logistic Regression, as it achieved the highest F1-score and accuracy among the three evaluated models, as shown in the comparative performance charts. Logistic Regression recorded an F1-score close to 0.89 and an accuracy of approximately 0.89, outperforming Support Vector Machine (SVM) and Multinomial Naive Bayes. Given the imbalanced nature of sentiment classes, F1-score was prioritized, making Logistic Regression the most reliable and balanced model for this task. In addition to its strong predictive performance, Logistic Regression offers high interpretability, which is critical for business-oriented sentiment analysis."
      ],
      "metadata": {
        "id": "H2bjrLuJ4Q6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "HE0p2zf5BHuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ---------------- Logistic Regression ----------------\n",
        "lr_accuracy = accuracy_score(y_test, y_test_pred_lr_tuned)\n",
        "lr_precision = precision_score(y_test, y_test_pred_lr_tuned, average='weighted')\n",
        "lr_recall = recall_score(y_test, y_test_pred_lr_tuned, average='weighted')\n",
        "lr_f1 = f1_score(y_test, y_test_pred_lr_tuned, average='weighted')\n",
        "\n",
        "# ---------------- SVM ----------------\n",
        "svm_accuracy = accuracy_score(y_test, y_test_pred_svm_tuned)\n",
        "svm_precision = precision_score(y_test, y_test_pred_svm_tuned, average='weighted')\n",
        "svm_recall = recall_score(y_test, y_test_pred_svm_tuned, average='weighted')\n",
        "svm_f1 = f1_score(y_test, y_test_pred_svm_tuned, average='weighted')\n",
        "\n",
        "# ---------------- Naive Bayes ----------------\n",
        "nb_accuracy = accuracy_score(y_test_nb, y_test_pred_nb_tuned)\n",
        "nb_precision = precision_score(y_test_nb, y_test_pred_nb_tuned, average='weighted')\n",
        "nb_recall = recall_score(y_test_nb, y_test_pred_nb_tuned, average='weighted')\n",
        "nb_f1 = f1_score(y_test_nb, y_test_pred_nb_tuned, average='weighted')\n",
        "\n",
        "print(\"All model metrics computed and stored successfully.\")\n"
      ],
      "metadata": {
        "id": "aj88590qE7SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_performance = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'SVM', 'Naive Bayes'],\n",
        "    'Accuracy': [lr_accuracy, svm_accuracy, nb_accuracy],\n",
        "    'Precision': [lr_precision, svm_precision, nb_precision],\n",
        "    'Recall': [lr_recall, svm_recall, nb_recall],\n",
        "    'F1-Score': [lr_f1, svm_f1, nb_f1]\n",
        "})\n"
      ],
      "metadata": {
        "id": "c9diKxBBE8eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model-wise Performance Comparison (Bar Chart)"
      ],
      "metadata": {
        "id": "WSbaJgttFKjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Model Performance Comparison\n",
        "# ==========================================\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_performance = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'SVM', 'Naive Bayes'],\n",
        "    'Accuracy': [lr_accuracy, svm_accuracy, nb_accuracy],\n",
        "    'Precision': [lr_precision, svm_precision, nb_precision],\n",
        "    'Recall': [lr_recall, svm_recall, nb_recall],\n",
        "    'F1-Score': [lr_f1, svm_f1, nb_f1]\n",
        "})\n",
        "\n",
        "model_performance.set_index('Model').plot(\n",
        "    kind='bar',\n",
        "    figsize=(10,6)\n",
        ")\n",
        "\n",
        "plt.title(\"ML Model Performance Comparison\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylim(0,1)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5djwWqbqFEg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model-wise Performance Comparison (Bar Chart)"
      ],
      "metadata": {
        "id": "u-b62d8VFP4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# F1-Score Comparison\n",
        "# ==========================================\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "\n",
        "plt.bar(\n",
        "    model_performance['Model'],\n",
        "    model_performance['F1-Score']\n",
        ")\n",
        "\n",
        "plt.title(\"F1-Score Comparison Across Models\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylim(0,1)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tkbFGBuyFGq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "\n",
        "plt.bar(\n",
        "    model_performance['Model'],\n",
        "    model_performance['Accuracy']\n",
        ")\n",
        "\n",
        "plt.title(\"Accuracy Comparison Across Models\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylim(0,1)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZrRApOB32__d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Confusion Matrix Comparison (SVM vs Naive Bayes)"
      ],
      "metadata": {
        "id": "NCnJlnl7FenM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Confusion Matrix Comparison\n",
        "# ==========================================\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
        "\n",
        "cm_svm = confusion_matrix(y_test, y_test_pred_svm)\n",
        "cm_nb = confusion_matrix(y_test_nb, y_test_pred_nb_tuned)\n",
        "\n",
        "sns.heatmap(cm_svm, annot=True, fmt='d', ax=axes[0])\n",
        "axes[0].set_title(\"SVM Confusion Matrix\")\n",
        "axes[0].set_xlabel(\"Predicted\")\n",
        "axes[0].set_ylabel(\"Actual\")\n",
        "\n",
        "sns.heatmap(cm_nb, annot=True, fmt='d', ax=axes[1])\n",
        "axes[1].set_title(\"Naive Bayes Confusion Matrix\")\n",
        "axes[1].set_xlabel(\"Predicted\")\n",
        "axes[1].set_ylabel(\"Actual\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ICNeHYjPFbJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Save the Best Performing ML Model\n",
        "# ==========================================\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Save the final Logistic Regression model\n",
        "joblib.dump(best_svm_model, 'zomato_sentiment_logistic_regression_model.joblib')\n",
        "\n",
        "print(\"Best performing Logistic Regression model saved successfully as 'zomato_sentiment_logistic_regression_model.joblib'\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Load Saved Model & Predict Unseen Data\n",
        "# ==========================================\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Load the saved Logistic Regression model\n",
        "loaded_lr_model = joblib.load('zomato_sentiment_logistic_regression_model.joblib')\n",
        "\n",
        "print(\"Saved Logistic Regression model loaded successfully.\")\n",
        "\n",
        "# Select unseen data samples from test set\n",
        "X_unseen = X_test[:10]\n",
        "y_actual = y_test[:10]\n",
        "\n",
        "# Predict sentiment on unseen data\n",
        "y_pred_unseen = loaded_lr_model.predict(X_unseen)\n",
        "\n",
        "# Display actual vs predicted results\n",
        "for i in range(len(y_pred_unseen)):\n",
        "    print(f\"Sample {i+1} - Actual: {y_actual.iloc[i]} | Predicted: {y_pred_unseen[i]}\")\n"
      ],
      "metadata": {
        "id": "dMgXZ3Zx55OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BEST MODEL - LOGISTIC REGRESSION**"
      ],
      "metadata": {
        "id": "RkBYgEyv6t58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully implemented an end-to-end machine learning pipeline for restaurant sentiment analysis using Zomato customer review data, with a clear focus on extracting actionable insights for both customers and businesses. Through detailed exploratory data analysis, the project identified key behavioral patterns such as the dominance of positive customer reviews, the measurable impact of customer engagement (pictures uploaded and review depth) on sentiment, and the strong relationship between rating consistency and overall customer satisfaction. These findings validated the projectâ€™s objective of helping customers identify the best-performing restaurants while enabling companies to recognize operational gaps.\n",
        "\n",
        "\n",
        "Comprehensive data preprocessing and feature engineering were performed to ensure analytical robustness. This included missing value imputation, IQR-based outlier treatment, categorical encoding, and an extensive NLP pipeline involving text normalization, tokenization, lemmatization, and TFâ€“IDF vectorization. The TFâ€“IDF feature space, combined with dimensionality reduction and feature selection, effectively captured sentiment-bearing terms while maintaining computational efficiency. Class imbalance was successfully addressed using SMOTE, ensuring that negative and neutral sentiments were learned fairly alongside the dominant positive class.\n",
        "\n",
        "\n",
        "Multiple machine learning modelsâ€”Logistic Regression, Support Vector Machine, and Multinomial Naive Bayesâ€”were trained and evaluated using accuracy and F1-score as primary metrics. Logistic Regression emerged as the best-performing model, achieving an accuracy of approximately 0.89 and an F1-score close to 0.89, outperforming SVM and Naive Bayes in both stability and class-wise balance. Its superior performance, combined with high interpretability, made it the most suitable model for this sentiment classification task. Coefficient-based explainability further enabled identification of key positive and negative sentiment-driving terms, translating model predictions into meaningful business insights.\n",
        "\n",
        "\n",
        "Advanced analytical extensions such as costâ€“benefit analysis, restaurant segmentation, and critic identification provided additional value. The analysis revealed that higher cost does not necessarily guarantee higher satisfaction, emphasizing the importance of service quality and consistency. Influential reviewers were identified using engagement metadata, offering the platform an opportunity to monitor sentiment leaders and improve trust-based ranking mechanisms.\n",
        "\n",
        "\n",
        "Finally, the Logistic Regression model was saved, reloaded, and validated on unseen data, confirming deployment readiness and reproducibility. Overall, this project successfully met all defined objectives, delivering a production-ready, interpretable, and business-relevant sentiment analysis system. The insights generated can support better restaurant recommendations, pricing strategies, customer experience optimization, and informed decision-making, demonstrating a professionally executed and academically sound machine learning capstone project."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}